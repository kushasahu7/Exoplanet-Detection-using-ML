{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-11T16:44:25.104014Z",
     "start_time": "2024-11-11T16:44:17.286094Z"
    }
   },
   "source": [
    "from astropy.io import fits\n",
    "import antropy as ant\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.src.utils.module_utils import scipy\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.signal import find_peaks, periodogram\n",
    "from scipy.fft import fft\n",
    "import tensorflow as tf\n",
    "#import tsfel\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.model_selection import KFold\n",
    "from tsfresh import extract_relevant_features\n",
    "from tsfresh.feature_extraction import EfficientFCParameters\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(tf.config.list_physical_devices())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T16:44:25.206968Z",
     "start_time": "2024-11-11T16:44:25.200772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_features(time_series, sampling_rate=1.0):\n",
    "    \"\"\"\n",
    "    Extracts a set of temporal and spectral features from a given time series.\n",
    "    \n",
    "    Parameters:\n",
    "    - time_series: np.array, the input time series data\n",
    "    - sampling_rate: float, the sampling rate of the time series (default is 1.0 Hz)\n",
    "    \n",
    "    Returns:\n",
    "    - features: np.array, an array of extracted feature values\n",
    "    \"\"\"\n",
    "    # Ensure the input is a numpy array\n",
    "    time_series = np.array(time_series)\n",
    "\n",
    "    # Temporal Features\n",
    "    mean_value = np.mean(time_series)\n",
    "    std_value = np.std(time_series)\n",
    "    median_flux = np.median(time_series)\n",
    "    min_flux = np.min(time_series)\n",
    "    max_flux = np.max(time_series)\n",
    "    skewness = skew(time_series)\n",
    "    kurt = kurtosis(time_series)\n",
    "    peak_to_peak = np.ptp(-time_series)  # Peak-to-peak amplitude\n",
    "    median_absolute_deviation = np.median(np.abs(time_series - np.median(time_series)))\n",
    "\n",
    "    # Number of significant peaks\n",
    "    peaks, _ = find_peaks(time_series)\n",
    "    num_peaks = len(peaks)\n",
    "    \n",
    "    dips, _ = find_peaks(-time_series)\n",
    "    num_dips = len(dips)\n",
    "    if len(dips) > 1:\n",
    "        duration_of_dips = np.mean(np.diff(dips))\n",
    "    else:\n",
    "        duration_of_dips = 0\n",
    "\n",
    "    # Autocorrelation (lag-1)\n",
    "    if len(time_series) > 1:\n",
    "        autocorr = np.corrcoef(time_series[:-1], time_series[1:])[0, 1]\n",
    "    else:\n",
    "        autocorr = 0\n",
    "\n",
    "    # Slope of the time series\n",
    "    slope = (time_series[-1] - time_series[0]) / len(time_series)\n",
    "    # zero_crossings = np.where(np.diff(np.sign(time_series)))[0]\n",
    "    # zero_crossing_rate = len(zero_crossings) / len(time_series)\n",
    "    \n",
    "    # Spectral Features\n",
    "    # Compute the Power Spectral Density (PSD) using periodogram\n",
    "    freqs, power = periodogram(time_series, fs=sampling_rate)\n",
    "\n",
    "    # Dominant frequency and its power\n",
    "    if len(freqs) > 1:\n",
    "        dominant_freq = freqs[np.argmax(power)]\n",
    "        power_of_dominant_freq = np.max(power)\n",
    "    else:\n",
    "        dominant_freq = 0\n",
    "        power_of_dominant_freq = 0\n",
    "\n",
    "    # Combine all features into a single array\n",
    "    features = np.array([\n",
    "        mean_value, std_value, skewness, kurt, peak_to_peak,\n",
    "        median_absolute_deviation, num_peaks, autocorr, \n",
    "        #slope,\n",
    "        dominant_freq, \n",
    "        #power_of_dominant_freq, \n",
    "        num_dips,median_flux,min_flux,max_flux,duration_of_dips\n",
    "    ])\n",
    "\n",
    "    return features"
   ],
   "id": "cb2f632fb329a970",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T16:44:25.222986Z",
     "start_time": "2024-11-11T16:44:25.214492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_spectral_features(light_curve, sampling_rate=1.0):\n",
    "    light_curve = np.array(light_curve)\n",
    "    freqs, power = periodogram(light_curve, fs=sampling_rate)\n",
    "    \n",
    "    # Spectrogram Mean Coefficient\n",
    "    spectrogram_mean = np.mean(power)\n",
    "    \n",
    "    # Fundamental Frequency and Max Power Spectrum\n",
    "    if len(freqs) > 1:\n",
    "        fundamental_frequency = freqs[np.argmax(power)]\n",
    "        max_power_spectrum = np.max(power)\n",
    "    else:\n",
    "        fundamental_frequency = 0\n",
    "        max_power_spectrum = 0\n",
    "    \n",
    "    # Spectral Centroid\n",
    "    spectral_centroid = np.sum(freqs * power) / np.sum(power) if np.sum(power) > 0 else 0\n",
    "    \n",
    "    # Spectral Entropy\n",
    "    normalized_power = power / np.sum(power) if np.sum(power) > 0 else power\n",
    "    spectral_entropy = -np.sum(normalized_power * np.log2(normalized_power + 1e-10))\n",
    "    \n",
    "    # Spectral Kurtosis\n",
    "    spectral_kurtosis = kurtosis(power)\n",
    "    \n",
    "    # Spectral Skewness\n",
    "    spectral_skewness = skew(power)\n",
    "    \n",
    "    # Spectral Roll-off (85% energy)\n",
    "    cumulative_energy = np.cumsum(power)\n",
    "    spectral_roll_off = freqs[np.where(cumulative_energy >= 0.85 * cumulative_energy[-1])[0][0]] if len(cumulative_energy) > 0 else 0\n",
    "    \n",
    "    # Spectral Slope\n",
    "    if len(freqs) > 1:\n",
    "        spectral_slope = (power[-1] - power[0]) / (freqs[-1] - freqs[0])\n",
    "    else:\n",
    "        spectral_slope = 0\n",
    "    \n",
    "    # Wavelet Features\n",
    "    wavelet = 'db1'  # Daubechies wavelet with one vanishing moment\n",
    "    coeffs = pywt.wavedec(light_curve, wavelet, level=3)\n",
    "    \n",
    "    # Wavelet Energy\n",
    "    wavelet_energy = np.sum([np.sum(c**2) for c in coeffs])\n",
    "    \n",
    "    # Wavelet Entropy\n",
    "    wavelet_coeffs = np.concatenate(coeffs)\n",
    "    normalized_wavelet_coeffs = wavelet_coeffs / np.sum(wavelet_coeffs) if np.sum(wavelet_coeffs) > 0 else wavelet_coeffs\n",
    "    # Filter out zero or negative values before computing the log\n",
    "    valid_coeffs = normalized_wavelet_coeffs[normalized_wavelet_coeffs > 0]\n",
    "    wavelet_entropy = -np.sum(valid_coeffs * np.log2(valid_coeffs + 1e-10))\n",
    "    \n",
    "    # Combine all features into a single array\n",
    "    features = np.array([\n",
    "        spectrogram_mean, fundamental_frequency, max_power_spectrum, #spectral_centroid,\n",
    "        spectral_entropy, spectral_kurtosis, spectral_skewness, spectral_roll_off,\n",
    "        #spectral_slope, \n",
    "        wavelet_energy, wavelet_entropy\n",
    "    ])\n",
    "    \n",
    "    return features\n",
    "def compute_additional_features(light_curve):\n",
    "    features = []\n",
    "\n",
    "    # Spectral Flatness\n",
    "    power_spectrum = np.abs(np.fft.fft(light_curve))**2\n",
    "    spectral_flatness = np.exp(np.mean(np.log(power_spectrum + 1e-10))) / np.mean(power_spectrum)\n",
    "    features.append(spectral_flatness)\n",
    "\n",
    "    # Lagged Autocorrelations\n",
    "    autocorr_lag_5 = np.corrcoef(light_curve[:-5], light_curve[5:])[0, 1]\n",
    "    features.append(autocorr_lag_5)\n",
    "\n",
    "    # Zero-Crossing Rate\n",
    "    zero_crossings = np.where(np.diff(np.sign(light_curve)))[0]\n",
    "    zero_crossing_rate = len(zero_crossings) / len(light_curve)\n",
    "    features.append(zero_crossing_rate)\n",
    "\n",
    "    # Approximate Entropy (using a simplified method)\n",
    "    def approximate_entropy(U, m, r):\n",
    "        N = len(U)\n",
    "        def _phi(m):\n",
    "            x = np.array([U[i : i + m] for i in range(N - m + 1)])\n",
    "            C = np.sum(np.max(np.abs(x[:, None] - x[None, :]), axis=2) <= r, axis=0) / (N - m + 1)\n",
    "            return np.sum(np.log(C)) / (N - m + 1)\n",
    "        return abs(_phi(m + 1) - _phi(m))\n",
    "\n",
    "    approx_entropy = ant.app_entropy(light_curve, order=2, metric='chebyshev')\n",
    "    #approx_entropy = approximate_entropy(light_curve, m=2, r=0.2 * np.std(light_curve))\n",
    "    features.append(approx_entropy)\n",
    "\n",
    "    return np.array(features)"
   ],
   "id": "b89cba2a5f3c6ccc",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T16:44:25.236064Z",
     "start_time": "2024-11-11T16:44:25.230259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_dip_depths(light_curve,time, prominence=1, distance=5 ):\n",
    "    baseline_flux = np.median(light_curve)  \n",
    "    inverted_light_curve = baseline_flux - light_curve\n",
    "    #inverted_light_curve = -light_curve\n",
    "    dips, properties = find_peaks(inverted_light_curve, prominence=prominence)\n",
    "    dip_depths = baseline_flux - light_curve[dips]\n",
    "    dip_durations = []\n",
    "    \n",
    "    negative_count = 0\n",
    "    for dip in dips:\n",
    "        # Find the start of the dip\n",
    "        start_index = dip\n",
    "        while start_index > 0 and light_curve[start_index] < baseline_flux:\n",
    "            start_index -= 1\n",
    "        end_index = dip\n",
    "        while end_index < len(light_curve) - 1 and light_curve[end_index] < baseline_flux:\n",
    "            end_index += 1\n",
    "        duration = time[end_index] - time[start_index]\n",
    "        if duration < 0:\n",
    "            negative_count += 1\n",
    "            duration = 1e-15\n",
    "        #print(time[start_index], time[end_index], duration)\n",
    "        dip_durations.append(duration)\n",
    "\n",
    "    return dip_depths, dips, dip_durations, True if negative_count == len(dips) else False\n",
    "    \n",
    "def dip_related_features(light_curve, time_array, sampling_interval):\n",
    "    dip_depths, dips, dip_durations, all_neg = get_dip_depths(light_curve, time_array)\n",
    "    num_dips = len(dips)\n",
    "    avg_dip_depth = 0 if all_neg else np.mean(dip_depths)\n",
    "    max_dip_depth = 0 if all_neg else np.max(dip_depths)\n",
    "    variance_dip_depth = 1e10 if all_neg else np.var(dip_depths)\n",
    "    avg_dip_duration = np.mean(dip_durations)\n",
    "    total_dip_duration = np.sum(dip_durations)\n",
    "    dip_distances = np.diff(dips)\n",
    "    dip_distances_in_time = dip_distances * sampling_interval\n",
    "    avg_distance = np.mean(dip_distances_in_time)\n",
    "    min_distance = np.min(dip_distances_in_time)\n",
    "    max_distance = np.max(dip_distances_in_time)\n",
    "    std_distance = np.std(dip_distances_in_time)\n",
    "    median_dist = np.median(dip_distances_in_time)\n",
    "    #dip_frequency = len(num_dips) / total_time\n",
    "    #dip_depth_duration_ratio = np.mean(dip_depths / dip_durations)\n",
    "    #dip_symmetry = np.mean(np.abs(rise_times - fall_times))\n",
    "    #avg_time_between_dips = np.mean(np.diff(dip_times))\n",
    "    skewness_dip_depth = scipy.stats.skew(dip_depths)\n",
    "    kurtosis_dip_depth = kurtosis(dip_depths)\n",
    "    return [num_dips, avg_dip_depth, max_dip_depth, variance_dip_depth, avg_dip_duration, total_dip_duration,skewness_dip_depth, kurtosis_dip_depth, avg_distance, min_distance, max_distance, std_distance,median_dist]\n",
    "    \n",
    "    "
   ],
   "id": "ac7375842aa34a81",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T16:44:25.253646Z",
     "start_time": "2024-11-11T16:44:25.247833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "index = 0\n",
    "positive_folder = \"./data/positive/\"\n",
    "negative_folder = \"./data/negative/\"\n",
    "flux_array = []\n",
    "time_array = []\n",
    "id_list = []\n",
    "labels = []\n",
    "count = 0\n",
    "# label_obj = {}\n",
    "start_of_0_ind = 0\n",
    "extracted_features_list = []\n",
    "def load_fits_data(folder, label):\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith('.fits'):\n",
    "            with fits.open(os.path.join(folder, filename)) as hdul:\n",
    "                try:\n",
    "                    global index\n",
    "                    data = hdul[1].data\n",
    "                    header = hdul[1].header\n",
    "                    time = data['TIME']\n",
    "                    flux = data['PDCSAP_FLUX']\n",
    "                    #print(\"flux\", flux.shape)\n",
    "                    obsmode = header.get('OBSMODE')\n",
    "                    timedel = header.get('TIMEDEL') * 86400\n",
    "                    sampling_rate = 1/timedel\n",
    "                    #print(obsmode, timedel, sampling_rate)\n",
    "\n",
    "                    valid_indices = ~np.isnan(time) & ~np.isnan(flux)\n",
    "                    time = time[valid_indices]\n",
    "                    flux = flux[valid_indices]\n",
    "                    # mean = np.mean(flux)\n",
    "                    # std = np.std(flux)\n",
    "                    # new_flux = (flux - mean) / std\n",
    "                    smoothed_flux = savgol_filter(flux, window_length=201, polyorder=6)\n",
    "                    #detrended_flux = flux - smoothed_flux\n",
    "                    \n",
    "                    #Needed for tsfresh\n",
    "                    # id_arr = np.array([index for i in range(flux.shape[0])])\n",
    "                    # time_array.extend(time)\n",
    "                    # flux_array.extend(flux)\n",
    "                    # id_list.extend(id_arr)\n",
    "                    \n",
    "                    # label_obj[index] = label\n",
    "                    \n",
    "                    # basic_features = extract_features(smoothed_flux, sampling_rate)\n",
    "                    spectral_features = extract_spectral_features(smoothed_flux, sampling_rate)\n",
    "                    # # #detrend_features = extract_features(detrended_flux, sampling_rate)\n",
    "                    additional_features =compute_additional_features(smoothed_flux)\n",
    "                    dip_features = dip_related_features(smoothed_flux, time,timedel)\n",
    "                    #print(basic_features)\n",
    "                    X = np.concatenate((\n",
    "                        #basic_features, \n",
    "                        spectral_features,\n",
    "                        additional_features,dip_features))\n",
    "                    #X = np.array(dip_features)\n",
    "                    extracted_features_list.append(X)\n",
    "                    # print(len(basic_features), len(spectral_features), len(additional_features), len(dip_features))\n",
    "                    #print(dip_features)\n",
    "                    #n = len(time)\n",
    "                    \n",
    "                    #Needed for tsfresh\n",
    "                    # flux_array.extend(smoothed_flux)\n",
    "                    # time_array.extend(time)\n",
    "                    # id_list.extend([index] * n)\n",
    "                    # labels.extend([label] * n)\n",
    "                    index += 1\n",
    "                    labels.append(label)\n",
    "                    #print(X.shape)\n",
    "                except ValueError as e:\n",
    "                    print(\"ValueError error for filename\", e)\n",
    "                except AttributeError as e:\n",
    "                    print(\"AttributeError error for filename\", filename, e)\n",
    "                except TypeError as e:\n",
    "                    print(\"TypeError error for filename\", filename, e)\n",
    "                except NameError as e:\n",
    "                    print(\"NameError error for filename\", filename, e)\n",
    "                except Exception as ex:\n",
    "                    print(\"got error for filename\", filename, index)\n",
    "                    print(\"Unexpected error:\", sys.exc_info()[0])"
   ],
   "id": "5b811ff5166b31a5",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-11T16:44:25.280866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#index=1\n",
    "load_fits_data(positive_folder, label=1)\n",
    "#index =1\n",
    "load_fits_data(negative_folder, label=0)\n",
    "print(\"start_of_0_ind\",start_of_0_ind)\n",
    "\n",
    "\n",
    "\n",
    "#"
   ],
   "id": "d62ebc7234b863be",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: File may have been truncated: actual file length (442368) is smaller than the expected size (463680) [astropy.io.fits.file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TypeError error for filename kplr002713049-2012179063303_llc.fits buffer is too small for requested array\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## TSFRESH BLOCK\n",
    "# df = pd.DataFrame({\n",
    "#     'time': time_array,\n",
    "#     'id': id_list,\n",
    "#     'flux': flux_array\n",
    "# })\n",
    "# print(df.dtypes)\n",
    "# print(df.isnull().sum())\n",
    "# print(len(labels), len(id_list), len(flux_array), len(time_array))\n",
    "# label_series = pd.Series(np.array(labels))\n",
    "# print(max(id_list))\n",
    "# missing_ids = set(df['id'].unique()) - set(label_series.index)\n",
    "# print(\"Missing IDs:\", missing_ids)\n",
    "from tsfresh.utilities.distribution import MultiprocessingDistributor\n",
    "# Distributor = MultiprocessingDistributor(n_workers=4,\n",
    "#                                          disable_progressbar=False,\n",
    "#                                          progressbar_title=\"Feature Extraction\")\n",
    "# extracted_features = extract_relevant_features(\n",
    "#     df,\n",
    "#     y=label_series,\n",
    "#     column_id=\"id\",\n",
    "#     column_sort=\"time\",\n",
    "#     column_value=\"flux\",\n",
    "#     #distributor=Distributor,\n",
    "#     #n_jobs=2  # Use all available cores\n",
    "# )\n",
    "# from tsfresh.utilities.distribution import MultiprocessingDistributor\n",
    "# Distributor = MultiprocessingDistributor(n_workers=4,\n",
    "#                                          disable_progressbar=False,\n",
    "#                                          progressbar_title=\"Feature Extraction\")\n",
    "# sample_df = df.sample(n=500, random_state=42)\n",
    "# sample_ids = []\n",
    "# sample_ids.extend(range(0,50))\n",
    "# sample_ids.extend(range(60242829,60242828+51))#sample_df['id'].unique()  # Get unique ids from the sample\n",
    "# print(sample_ids)\n",
    "#filtered_df = df[df['id'].isin(sample_ids)]  # Filter DataFrame to only include these ids\n",
    "#labels_series_sample = label_series[label_series.index.isin(sample_ids)]\n",
    "#print(\"labels_series_sample\",labels_series_sample)\n",
    "# extracted_features = extract_relevant_features(\n",
    "#     df,\n",
    "#     y=label_series,\n",
    "#     column_id=\"id\",\n",
    "#     column_sort=\"time\",\n",
    "#     column_value=\"flux\",\n",
    "#     #distributor=Distributor,\n",
    "#     n_jobs=-1  # Use all available cores\n",
    "# )\n",
    "# print(extracted_features.shape)\n",
    "#extracted_features.to_csv('extracted_features.csv')\n",
    "# print(extracted_features.shape)"
   ],
   "id": "6967379445fb6ca1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(len(extracted_features_list), len(labels), index)\n",
    "# Feature 25: 0.1067\n",
    "# Feature 27: 0.0736\n",
    "# Feature 10: 0.0657\n",
    "# Feature 24: 0.0586\n",
    "# Feature 12: 0.0564\n",
    "# Feature 14: 0.0543\n",
    "# Feature 7: 0.0536\n",
    "# Feature 28: 0.0477\n",
    "# Feature 32: 0.0444\n",
    "# Feature 9: 0.0399\n",
    "# Feature 16: 0.0370\n",
    "# Feature 1: 0.0367\n",
    "# Feature 11: 0.0326\n",
    "# Feature 15: 0.0308\n",
    "# Feature 8: 0.0307\n",
    "# Feature 5: 0.0292\n",
    "# Feature 13: 0.0284\n",
    "# Feature 22: 0.0274\n",
    "# Feature 23: 0.0200\n",
    "# Feature 3: 0.0189\n",
    "# Feature 21: 0.0181\n",
    "# Feature 29: 0.0167\n",
    "# Feature 6: 0.0125\n",
    "extracted_features_list = np.array(extracted_features_list)\n",
    "# indexes = [25,27,10,24,12,14,7,28,32,9,16,11,1,15,8,5,13,22,23,3,21,29,6]\n",
    "# indexes.sort()\n",
    "# print(\"len\", len(indexes), extracted_features_list.shape)\n",
    "# extracted_features_list = extracted_features_list[:,indexes]\n",
    "scaler = StandardScaler()\n",
    "extracted_features_list = scaler.fit_transform(extracted_features_list)\n",
    "#print(\"extracted_features_list normaised\", extracted_features_list)\n",
    "labels = np.array(labels)\n",
    "x, x_test, y, y_test = train_test_split(extracted_features_list, labels, test_size=0.2)\n",
    "print(extracted_features_list.shape, x.shape, x_test.shape, y.shape, y_test.shape)"
   ],
   "id": "ed3ef756d35e6fd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_classifier():\n",
    "    #model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    #model = LogisticRegression(random_state=42)\n",
    "    # model = SVC(kernel='linear', random_state=42)\n",
    "    # model = GradientBoostingClassifier(n_estimators=700, random_state=2, learning_rate=0.01, max_depth=3, subsample=0.8, min_samples_leaf=2, min_samples_split=2)\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=1500,  # Number of trees (increase with lower learning rate)\n",
    "        learning_rate=0.001,  # Lower learning rate for better generalization\n",
    "        max_depth=10,  # Control tree depth (reduce to prevent overfitting)\n",
    "        subsample=0.9,  # Use 80% of training samples for each tree\n",
    "        colsample_bytree=0.9,  # Use 80% of features for each tree\n",
    "        min_child_weight=50,  # Minimum sum of instance weight (controls complexity)\n",
    "        gamma=0.005,  # Minimum loss reduction (regularization)\n",
    "        #objective='binary:logistic',  # Suitable for binary classification\n",
    "        eval_metric='auc',  # Metric to evaluate (logarithmic loss)\n",
    "        use_label_encoder=False,  # Disable label encoding for newer versions\n",
    "        random_state=42,  # Set a random state for reproducibility,\n",
    "        tree_method='exact'\n",
    "        #early_stopping_rounds=200\n",
    "    )\n",
    "    return model"
   ],
   "id": "dba8ea12335e1bc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# def get_nn_model():",
   "id": "4d7425f223869204",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_model(x, y, model):\n",
    "    kfold = KFold(n_splits=2, shuffle=True)\n",
    "    accuracy_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    train_accuracy_list = []\n",
    "    for train_index, validation_index in kfold.split(x):\n",
    "        X_train, X_val = x[train_index], x[validation_index]\n",
    "        y_train, y_val = y[train_index], y[validation_index]\n",
    "        eval_set = [(X_val, y_val)]\n",
    "        #model.fit(X_train, y_train)\n",
    "        \n",
    "        #fit for xgboost:\n",
    "        model.fit(X_train, y_train, eval_set=eval_set, verbose=True)\n",
    "        y_train_pred_prob = model.predict_proba(X_train)[:,1]\n",
    "        y_val_pred_prob = model.predict_proba(X_val)[:,1]\n",
    "        y_train_pred = (y_train_pred_prob > 0.48).astype(int)\n",
    "        y_val_pred = (y_val_pred_prob > 0.48).astype(int)\n",
    "        accuracy_list.append(accuracy_score(y_val, y_val_pred))\n",
    "        train_accuracy_list.append(accuracy_score(y_train, y_train_pred))\n",
    "        precision_list.append(precision_score(y_val, y_val_pred))\n",
    "        recall_list.append(recall_score(y_val, y_val_pred))\n",
    "    return accuracy_list, precision_list, recall_list,train_accuracy_list\n"
   ],
   "id": "5f14bea0dc85fc9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def predict(model, x, y):\n",
    "    y_pred_prob = model.predict_proba(x)[:,1]\n",
    "    y_pred = (y_pred_prob > 0.48).astype(int)\n",
    "    test_accuracy = accuracy_score(y, y_pred)\n",
    "    test_precision = precision_score(y, y_pred)\n",
    "    test_recall = recall_score(y, y_pred)\n",
    "    return test_accuracy, test_precision, test_recall"
   ],
   "id": "3449d1136ef8de5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = get_classifier()\n",
    "accuracy_list, precision_list, recall_list, train_accuracy_list = run_model(x, y, model)\n",
    "feature_importances = model.feature_importances_\n",
    "## for importance of xgbost\n",
    "# xgb.plot_importance(model, importance_type=\"weight\")\n",
    "# plt.show()\n",
    "\n",
    "# Sort and display feature importances\n",
    "sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "print(\"Feature Importances:\")\n",
    "for idx in sorted_indices:\n",
    "    print(f\"Feature {idx + 1}: {feature_importances[idx]:.4f}\")\n",
    "test_accuracy, test_precision, test_recall = predict(model, x_test, y_test)"
   ],
   "id": "73be8ef88c145077",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(test_accuracy, test_precision, test_recall)\n",
    "plt.plot(train_accuracy_list)\n",
    "plt.plot(accuracy_list)\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ],
   "id": "e4197f403435e04b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "print(test_accuracy, test_precision, test_recall)"
   ],
   "id": "ed40ae7eb01c38ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "base_model = DecisionTreeClassifier()\n",
    "\n",
    "# Create BaggingClassifier\n",
    "bagging_model = BaggingClassifier(base_model, n_estimators=500, random_state=42)\n",
    "bagging_model.fit(x, y)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = bagging_model.predict(x_test)\n",
    "print(\"Bagging Accuracy:\", accuracy_score(y_test, y_pred))"
   ],
   "id": "ef2dd5876f2f4f26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "84cb5e5fb8576a87",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
