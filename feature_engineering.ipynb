{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-05T16:40:24.878649Z",
     "start_time": "2024-11-05T16:40:17.669783Z"
    }
   },
   "source": [
    "from astropy.io import fits\n",
    "import antropy as ant\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.signal import find_peaks, periodogram\n",
    "from scipy.fft import fft\n",
    "import tensorflow as tf\n",
    "import tsfel\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.model_selection import KFold\n",
    "# from tsfresh import extract_relevant_features\n",
    "# from tsfresh.feature_extraction import EfficientFCParameters\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, Flatten\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T16:40:25.004750Z",
     "start_time": "2024-11-05T16:40:24.889563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "positive_dips = []\n",
    "negative_dips = []\n",
    "def extract_features(time_series, sampling_rate=1.0, label=None):\n",
    "    \"\"\"\n",
    "    Extracts a set of temporal and spectral features from a given time series.\n",
    "    \n",
    "    Parameters:\n",
    "    - time_series: np.array, the input time series data\n",
    "    - sampling_rate: float, the sampling rate of the time series (default is 1.0 Hz)\n",
    "    \n",
    "    Returns:\n",
    "    - features: np.array, an array of extracted feature values\n",
    "    \"\"\"\n",
    "    # Ensure the input is a numpy array\n",
    "    time_series = np.array(time_series)\n",
    "\n",
    "    # Temporal Features\n",
    "    mean_value = np.mean(time_series)\n",
    "    std_value = np.std(time_series)\n",
    "    median_flux = np.median(time_series)\n",
    "    min_flux = np.min(time_series)\n",
    "    max_flux = np.max(time_series)\n",
    "    skewness = skew(time_series)\n",
    "    kurt = kurtosis(time_series)\n",
    "    peak_to_peak = np.ptp(time_series)  # Peak-to-peak amplitude\n",
    "    median_absolute_deviation = np.median(np.abs(time_series - np.median(time_series)))\n",
    "\n",
    "    # Number of significant peaks\n",
    "    peaks, _ = find_peaks(time_series)\n",
    "    num_peaks = len(peaks)\n",
    "\n",
    "    dips, _ = find_peaks(-time_series)\n",
    "    num_dips = len(dips)\n",
    "    if label == 1:\n",
    "        positive_dips.append(num_dips)\n",
    "    else:\n",
    "        negative_dips.append(num_dips)\n",
    "    if len(dips) > 1:\n",
    "        duration_of_dips = np.mean(np.diff(dips))\n",
    "    else:\n",
    "        duration_of_dips = 0\n",
    "\n",
    "    # Autocorrelation (lag-1)\n",
    "    if len(time_series) > 1:\n",
    "        autocorr = np.corrcoef(time_series[:-1], time_series[1:])[0, 1]\n",
    "    else:\n",
    "        autocorr = 0\n",
    "\n",
    "    # Slope of the time series\n",
    "    slope = (time_series[-1] - time_series[0]) / len(time_series)\n",
    "    zero_crossings = np.where(np.diff(np.sign(time_series)))[0]\n",
    "    zero_crossing_rate = len(zero_crossings) / len(time_series)\n",
    "\n",
    "    # Spectral Features\n",
    "    # Compute the Power Spectral Density (PSD) using periodogram\n",
    "    freqs, power = periodogram(time_series, fs=sampling_rate)\n",
    "\n",
    "    # Dominant frequency and its power\n",
    "    if len(freqs) > 1:\n",
    "        dominant_freq = freqs[np.argmax(power)]\n",
    "        power_of_dominant_freq = np.max(power)\n",
    "    else:\n",
    "        dominant_freq = 0\n",
    "        power_of_dominant_freq = 0\n",
    "\n",
    "    # Combine all features into a single array\n",
    "    features = np.array([\n",
    "        mean_value, std_value, skewness, kurt, peak_to_peak,\n",
    "        median_absolute_deviation, num_peaks, autocorr, slope,\n",
    "        dominant_freq, power_of_dominant_freq, num_dips, median_flux, min_flux, max_flux, duration_of_dips\n",
    "    ])\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_spectral_features(light_curve, sampling_rate=1.0):\n",
    "    light_curve = np.array(light_curve)\n",
    "    freqs, power = periodogram(light_curve, fs=sampling_rate)\n",
    "\n",
    "    # Spectrogram Mean Coefficient\n",
    "    spectrogram_mean = np.mean(power)\n",
    "\n",
    "    # Fundamental Frequency and Max Power Spectrum\n",
    "    if len(freqs) > 1:\n",
    "        fundamental_frequency = freqs[np.argmax(power)]\n",
    "        max_power_spectrum = np.max(power)\n",
    "    else:\n",
    "        fundamental_frequency = 0\n",
    "        max_power_spectrum = 0\n",
    "\n",
    "    # Spectral Centroid\n",
    "    spectral_centroid = np.sum(freqs * power) / np.sum(power) if np.sum(power) > 0 else 0\n",
    "\n",
    "    # Spectral Entropy\n",
    "    normalized_power = power / np.sum(power) if np.sum(power) > 0 else power\n",
    "    spectral_entropy = -np.sum(normalized_power * np.log2(normalized_power + 1e-10))\n",
    "\n",
    "    # Spectral Kurtosis\n",
    "    spectral_kurtosis = kurtosis(power)\n",
    "\n",
    "    # Spectral Skewness\n",
    "    spectral_skewness = skew(power)\n",
    "\n",
    "    # Spectral Roll-off (85% energy)\n",
    "    cumulative_energy = np.cumsum(power)\n",
    "    spectral_roll_off = freqs[np.where(cumulative_energy >= 0.85 * cumulative_energy[-1])[0][0]] if len(\n",
    "        cumulative_energy) > 0 else 0\n",
    "\n",
    "    # Spectral Slope\n",
    "    if len(freqs) > 1:\n",
    "        spectral_slope = (power[-1] - power[0]) / (freqs[-1] - freqs[0])\n",
    "    else:\n",
    "        spectral_slope = 0\n",
    "\n",
    "    # Wavelet Features\n",
    "    wavelet = 'db1'  # Daubechies wavelet with one vanishing moment\n",
    "    coeffs = pywt.wavedec(light_curve, wavelet, level=3)\n",
    "\n",
    "    # Wavelet Energy\n",
    "    wavelet_energy = np.sum([np.sum(c ** 2) for c in coeffs])\n",
    "\n",
    "    # Wavelet Entropy\n",
    "    wavelet_coeffs = np.concatenate(coeffs)\n",
    "    normalized_wavelet_coeffs = wavelet_coeffs / np.sum(wavelet_coeffs) if np.sum(\n",
    "        wavelet_coeffs) > 0 else wavelet_coeffs\n",
    "    # Filter out zero or negative values before computing the log\n",
    "    valid_coeffs = normalized_wavelet_coeffs[normalized_wavelet_coeffs > 0]\n",
    "    wavelet_entropy = -np.sum(valid_coeffs * np.log2(valid_coeffs + 1e-10))\n",
    "\n",
    "    # Combine all features into a single array\n",
    "    features = np.array([\n",
    "        spectrogram_mean, fundamental_frequency, max_power_spectrum, spectral_centroid,\n",
    "        spectral_entropy, spectral_kurtosis, spectral_skewness, spectral_roll_off,\n",
    "        spectral_slope, wavelet_energy, wavelet_entropy\n",
    "    ])\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def compute_additional_features(light_curve):\n",
    "    features = []\n",
    "\n",
    "    # Spectral Flatness\n",
    "    power_spectrum = np.abs(np.fft.fft(light_curve)) ** 2\n",
    "    #print(\"done 1\")\n",
    "    spectral_flatness = np.exp(np.mean(np.log(power_spectrum + 1e-10))) / np.mean(power_spectrum)\n",
    "    #print(\"done 2\")\n",
    "    features.append(spectral_flatness)\n",
    "\n",
    "    # Lagged Autocorrelations\n",
    "    autocorr_lag_5 = np.corrcoef(light_curve[:-5], light_curve[5:])[0, 1]\n",
    "    features.append(autocorr_lag_5)\n",
    "    #print(\"done 3\")\n",
    "\n",
    "    # Zero-Crossing Rate\n",
    "    zero_crossings = np.where(np.diff(np.sign(light_curve)))[0]\n",
    "    zero_crossing_rate = len(zero_crossings) / len(light_curve)\n",
    "    features.append(zero_crossing_rate)\n",
    "    #print(\"done 4\")\n",
    "\n",
    "    # Approximate Entropy (using a simplified method)\n",
    "    def approximate_entropy(U, m, r):\n",
    "        N = len(U)\n",
    "\n",
    "        def _phi(m):\n",
    "            x = np.array([U[i: i + m] for i in range(N - m + 1)])\n",
    "            C = np.sum(np.max(np.abs(x[:, None] - x[None, :]), axis=2) <= r, axis=0) / (N - m + 1)\n",
    "            return np.sum(np.log(C)) / (N - m + 1)\n",
    "\n",
    "        return abs(_phi(m + 1) - _phi(m))\n",
    "\n",
    "    approx_entropy = ant.app_entropy(light_curve, order=2, metric='chebyshev')\n",
    "    #approx_entropy = approximate_entropy(light_curve, m=2, r=0.2 * np.std(light_curve))\n",
    "    #print(\"done 5\")\n",
    "    features.append(approx_entropy)\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "def features_from_tsfel(light_curve, sampling_rate=1.0):\n",
    "    lempel_ziv = tsfel.feature_extraction.features.lempel_ziv(light_curve)\n",
    "    maximum_fractal_length = tsfel.feature_extraction.features.maximum_fractal_length(light_curve)\n",
    "    return [lempel_ziv, maximum_fractal_length]\n"
   ],
   "id": "6fdc73c39d6c56d5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T16:40:25.051151Z",
     "start_time": "2024-11-05T16:40:25.045584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "index = 1\n",
    "positive_folder = \"./data/positive/\"\n",
    "negative_folder = \"./data/negative/\"\n",
    "# flux_array = []\n",
    "# time_array = []\n",
    "# id_list = []\n",
    "labels = []\n",
    "# label_obj = {}\n",
    "extracted_features_list = []\n",
    "\n",
    "\n",
    "def load_fits_data(folder, label):\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith('.fits'):\n",
    "            with fits.open(os.path.join(folder, filename)) as hdul:\n",
    "                try:\n",
    "                    global index\n",
    "                    if index > 100:\n",
    "                        break\n",
    "                    data = hdul[1].data\n",
    "                    header = hdul[1].header\n",
    "                    #print(header)\n",
    "                    time = data['TIME']\n",
    "                    flux = data['PDCSAP_FLUX']\n",
    "                    #print(\"flux\", flux.shape)\n",
    "                    obsmode = header.get('OBSMODE')\n",
    "                    timedel = header.get('TIMEDEL') * 86400\n",
    "                    sampling_rate = 1 / timedel\n",
    "                    #print(obsmode, timedel, sampling_rate)\n",
    "\n",
    "                    valid_indices = ~np.isnan(time) & ~np.isnan(flux)\n",
    "                    time = time[valid_indices]\n",
    "                    flux = flux[valid_indices]\n",
    "                    # mean = np.mean(flux)\n",
    "                    # std = np.std(flux)\n",
    "                    # new_flux = (flux - mean) / std\n",
    "                    smoothed_flux = savgol_filter(flux, window_length=201, polyorder=6)\n",
    "                    detrended_flux = flux - smoothed_flux\n",
    "\n",
    "                    #print(\"yoyoy\")\n",
    "                    # id_arr = np.array([index for i in range(flux.shape[0])])\n",
    "                    # time_array.extend(time)\n",
    "                    # flux_array.extend(flux)\n",
    "                    # id_list.extend(id_arr)\n",
    "                    # label_obj[index] = label\n",
    "                    index += 1\n",
    "                    # labels.append(label)\n",
    "                    basic_features = extract_features(smoothed_flux, sampling_rate, label)\n",
    "                    print(\"label, dips\", label, basic_features[-5])\n",
    "                    spectral_features = extract_spectral_features(smoothed_flux, sampling_rate)\n",
    "                    #detrend_features = extract_features(detrended_flux, sampling_rate)\n",
    "                    additional_features = compute_additional_features(smoothed_flux)\n",
    "                    #from_tsfel = features_from_tsfel(smoothed_flux, sampling_rate)\n",
    "                    #print(from_tsfel)\n",
    "                    #print(basic_features)\n",
    "                    X = np.concatenate((basic_features, spectral_features, additional_features))\n",
    "                    # print(len(basic_features), len(spectral_features), len(additional_features))\n",
    "                    #X.shape\n",
    "                    #cfg = tsfel.get_features_by_domain('spectral')\n",
    "                    #print(cfg)\n",
    "                    # df = pd.DataFrame({\n",
    "                    #     'time': time,\n",
    "                    #     'flux': smoothed_flux\n",
    "                    # })\n",
    "                    #X = tsfel.time_series_features_extractor(cfg, df, fs=0.056)\n",
    "                    extracted_features_list.append(X)\n",
    "                    labels.append(label)\n",
    "                    #print(X.shape)\n",
    "                except ValueError as e:\n",
    "                    print(\"ValueError error for filename\", e)\n",
    "                except AttributeError as e:\n",
    "                    print(\"AttributeError error for filename\", filename, e)\n",
    "                except NameError as e:\n",
    "                    print(\"NameError error for filename\", filename, e)\n",
    "                except Exception as ex:\n",
    "                    print(\"got error for filename\", filename, index)\n",
    "                    print(\"Unexpected error:\", sys.exc_info()[0])"
   ],
   "id": "babb38bdffb65091",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T16:40:57.588096Z",
     "start_time": "2024-11-05T16:40:25.056277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_fits_data(positive_folder, label=1)\n",
    "index = 1\n",
    "print(\"NOWWWWWWWWW LABEL=0\")\n",
    "load_fits_data(negative_folder, label=0)\n",
    "print(\"positive\", np.mean(positive_dips), np.median(positive_dips), np.std(positive_dips))\n",
    "print(\"negative\", np.mean(negative_dips), np.median(negative_dips))"
   ],
   "id": "6141537be634d71a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label, dips 1 8684.0\n",
      "label, dips 1 798.0\n",
      "label, dips 1 9490.0\n",
      "label, dips 1 6472.0\n",
      "label, dips 1 39.0\n",
      "label, dips 1 8723.0\n",
      "label, dips 1 5948.0\n",
      "label, dips 1 12.0\n",
      "label, dips 1 900.0\n",
      "label, dips 1 8140.0\n",
      "label, dips 1 8607.0\n",
      "label, dips 1 914.0\n",
      "label, dips 1 550.0\n",
      "label, dips 1 626.0\n",
      "label, dips 1 909.0\n",
      "label, dips 1 6633.0\n",
      "label, dips 1 288.0\n",
      "label, dips 1 685.0\n",
      "label, dips 1 873.0\n",
      "label, dips 1 154.0\n",
      "label, dips 1 293.0\n",
      "label, dips 1 569.0\n",
      "label, dips 1 6428.0\n",
      "label, dips 1 293.0\n",
      "label, dips 1 90.0\n",
      "label, dips 1 786.0\n",
      "label, dips 1 6588.0\n",
      "label, dips 1 8132.0\n",
      "label, dips 1 788.0\n",
      "label, dips 1 467.0\n",
      "label, dips 1 49.0\n",
      "label, dips 1 9870.0\n",
      "label, dips 1 558.0\n",
      "label, dips 1 348.0\n",
      "label, dips 1 800.0\n",
      "label, dips 1 278.0\n",
      "label, dips 1 801.0\n",
      "label, dips 1 9097.0\n",
      "label, dips 1 20.0\n",
      "label, dips 1 621.0\n",
      "label, dips 1 600.0\n",
      "label, dips 1 92.0\n",
      "label, dips 1 161.0\n",
      "label, dips 1 587.0\n",
      "label, dips 1 697.0\n",
      "label, dips 1 9123.0\n",
      "label, dips 1 7215.0\n",
      "label, dips 1 169.0\n",
      "label, dips 1 8239.0\n",
      "label, dips 1 738.0\n",
      "label, dips 1 939.0\n",
      "label, dips 1 855.0\n",
      "label, dips 1 580.0\n",
      "label, dips 1 718.0\n",
      "label, dips 1 639.0\n",
      "label, dips 1 299.0\n",
      "label, dips 1 169.0\n",
      "label, dips 1 212.0\n",
      "label, dips 1 468.0\n",
      "label, dips 1 787.0\n",
      "label, dips 1 626.0\n",
      "label, dips 1 9757.0\n",
      "label, dips 1 161.0\n",
      "label, dips 1 5744.0\n",
      "label, dips 1 539.0\n",
      "label, dips 1 868.0\n",
      "label, dips 1 478.0\n",
      "label, dips 1 803.0\n",
      "label, dips 1 282.0\n",
      "label, dips 1 73.0\n",
      "label, dips 1 8342.0\n",
      "label, dips 1 222.0\n",
      "label, dips 1 831.0\n",
      "label, dips 1 149.0\n",
      "label, dips 1 9337.0\n",
      "label, dips 1 178.0\n",
      "label, dips 1 7011.0\n",
      "label, dips 1 763.0\n",
      "label, dips 1 545.0\n",
      "label, dips 1 86.0\n",
      "label, dips 1 31.0\n",
      "label, dips 1 83.0\n",
      "label, dips 1 9752.0\n",
      "label, dips 1 782.0\n",
      "label, dips 1 506.0\n",
      "label, dips 1 935.0\n",
      "label, dips 1 941.0\n",
      "label, dips 1 620.0\n",
      "label, dips 1 9582.0\n",
      "label, dips 1 7178.0\n",
      "label, dips 1 170.0\n",
      "label, dips 1 615.0\n",
      "label, dips 1 4184.0\n",
      "label, dips 1 9698.0\n",
      "label, dips 1 11514.0\n",
      "label, dips 1 9849.0\n",
      "label, dips 1 507.0\n",
      "label, dips 1 840.0\n",
      "label, dips 1 899.0\n",
      "label, dips 1 602.0\n",
      "NOWWWWWWWWW LABEL=0\n",
      "label, dips 0 254.0\n",
      "label, dips 0 61.0\n",
      "label, dips 0 167.0\n",
      "label, dips 0 166.0\n",
      "label, dips 0 238.0\n",
      "label, dips 0 588.0\n",
      "label, dips 0 724.0\n",
      "label, dips 0 493.0\n",
      "label, dips 0 96.0\n",
      "label, dips 0 117.0\n",
      "label, dips 0 819.0\n",
      "label, dips 0 825.0\n",
      "label, dips 0 235.0\n",
      "label, dips 0 147.0\n",
      "label, dips 0 920.0\n",
      "label, dips 0 24.0\n",
      "label, dips 0 23.0\n",
      "label, dips 0 728.0\n",
      "label, dips 0 56.0\n",
      "label, dips 0 77.0\n",
      "label, dips 0 504.0\n",
      "label, dips 0 250.0\n",
      "label, dips 0 166.0\n",
      "label, dips 0 149.0\n",
      "label, dips 0 82.0\n",
      "label, dips 0 719.0\n",
      "label, dips 0 857.0\n",
      "label, dips 0 725.0\n",
      "label, dips 0 826.0\n",
      "label, dips 0 297.0\n",
      "label, dips 0 812.0\n",
      "label, dips 0 27.0\n",
      "label, dips 0 774.0\n",
      "label, dips 0 226.0\n",
      "label, dips 0 50.0\n",
      "label, dips 0 2161.0\n",
      "label, dips 0 349.0\n",
      "label, dips 0 203.0\n",
      "label, dips 0 88.0\n",
      "label, dips 0 613.0\n",
      "label, dips 0 223.0\n",
      "label, dips 0 124.0\n",
      "label, dips 0 100.0\n",
      "label, dips 0 6.0\n",
      "label, dips 0 167.0\n",
      "label, dips 0 503.0\n",
      "label, dips 0 644.0\n",
      "label, dips 0 780.0\n",
      "label, dips 0 768.0\n",
      "label, dips 0 641.0\n",
      "label, dips 0 850.0\n",
      "label, dips 0 521.0\n",
      "label, dips 0 803.0\n",
      "label, dips 0 75.0\n",
      "label, dips 0 194.0\n",
      "label, dips 0 121.0\n",
      "label, dips 0 91.0\n",
      "label, dips 0 702.0\n",
      "label, dips 0 875.0\n",
      "label, dips 0 248.0\n",
      "label, dips 0 8124.0\n",
      "label, dips 0 818.0\n",
      "label, dips 0 655.0\n",
      "label, dips 0 185.0\n",
      "label, dips 0 183.0\n",
      "label, dips 0 153.0\n",
      "label, dips 0 708.0\n",
      "label, dips 0 773.0\n",
      "label, dips 0 8117.0\n",
      "label, dips 0 213.0\n",
      "label, dips 0 305.0\n",
      "label, dips 0 234.0\n",
      "label, dips 0 723.0\n",
      "label, dips 0 819.0\n",
      "label, dips 0 240.0\n",
      "label, dips 0 833.0\n",
      "label, dips 0 319.0\n",
      "label, dips 0 176.0\n",
      "label, dips 0 82.0\n",
      "label, dips 0 903.0\n",
      "label, dips 0 65.0\n",
      "label, dips 0 13.0\n",
      "label, dips 0 138.0\n",
      "label, dips 0 116.0\n",
      "label, dips 0 843.0\n",
      "label, dips 0 143.0\n",
      "label, dips 0 822.0\n",
      "label, dips 0 312.0\n",
      "label, dips 0 49.0\n",
      "label, dips 0 885.0\n",
      "label, dips 0 638.0\n",
      "label, dips 0 604.0\n",
      "label, dips 0 508.0\n",
      "label, dips 0 574.0\n",
      "label, dips 0 70.0\n",
      "label, dips 0 37.0\n",
      "label, dips 0 75.0\n",
      "label, dips 0 99.0\n",
      "label, dips 0 297.0\n",
      "label, dips 0 6713.0\n",
      "positive 2656.91 728.0 3563.3400906873876\n",
      "negative 626.36 275.5\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bd7d0e78e42f1b17",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
